{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv4rjnwdW5XRtS+ANihRvo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkuSapphire/CreditRiskManagement/blob/main/Project_2_Time_Series_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Time Series Model\n",
        "\n",
        "By Bosen Li (bl3097), Wenyu Luo (wl2905), Edward Zhang (yz4756)"
      ],
      "metadata": {
        "id": "1QO0XIIwPhxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1\n",
        "\n",
        "Read the data into Python and convert them into charge-off percentages. Use the Augmented Dickie Fuller test to see if these series are stationary. If they are not, take the first differences."
      ],
      "metadata": {
        "id": "P9SFM005Qjdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzGzIck6Pezz",
        "outputId": "611518c8-56e2-446c-815f-6210b15f0615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is 0.489, indicating that this is not stationary.\n",
            "The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is 0.053, indicating that this is not stationary too.\n",
            "\n",
            "After taking the first difference,\n",
            "The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is 0.013, thus that this is stationary.\n",
            "The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is 0.002, thus that this is stationary too.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "cre = pd.read_excel('CRE.xlsx')\n",
        "cre.set_index('date',inplace=True)\n",
        "card = pd.read_excel('card.xlsx')\n",
        "card.set_index('date',inplace=True)\n",
        "\n",
        "cre['pct'] = 100 *  cre['chargeoffs']/cre['loans']\n",
        "card['pct'] = 100 * card['chargeoffs']/card['loans']\n",
        "\n",
        "print(f\"The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is \\\n",
        "{adfuller(cre['pct'])[1]:.3f}, \\\n",
        "indicating that this is not stationary.\")\n",
        "print(f\"The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is \\\n",
        "{adfuller(card['pct'])[1]:.3f}, \\\n",
        "indicating that this is not stationary too.\\n\")\n",
        "\n",
        "cre['pct_diff'] = cre['pct'].diff()\n",
        "card['pct_diff'] = card['pct'].diff()\n",
        "\n",
        "cre.dropna(inplace=True)\n",
        "card.dropna(inplace=True)\n",
        "\n",
        "print('After taking the first difference,')\n",
        "print(f\"The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is \\\n",
        "{adfuller(cre['pct_diff'])[1]:.3f}, \\\n",
        "thus that this is stationary.\")\n",
        "print(f\"The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is \\\n",
        "{adfuller(card['pct_diff'])[1]:.3f}, \\\n",
        "thus that this is stationary too.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "\n",
        "- Download unemployment data (UNRATE), oil prices (DCOILBRENTEU), US GDP (GDP), 10-year minus 2-year treasury rates (T10Y2Y), and a volatility series of your choice.\n",
        "- Pull the economic data into a pandas data frame.\n",
        "- Create a GDP growth variable (GDP_t – GDP_t-1) / GDP_t-1.\n",
        "- Combine this data with the charge-off data, making sure that none of the times in the economic series are later than the beginning dates for the charge-offs. (Clarification - each charge-off data point is measured over a full quarter. The reporting date is the end of the quarter. We want to ensure that the explanatory data does not overlap with the data point's quarter.)\n",
        "- Find the augmented Dickey-Fuller statistics for each economic time series. - If indicated, take the first difference and use that instead.\n",
        "- Run all possible AR1, three-factor models (one lag and three factors).\n",
        "- Choose the best model based on r-squared and comment on the results."
      ],
      "metadata": {
        "id": "nZSHv8zST6_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_datareader.data as web\n",
        "\n",
        "unrate = web.DataReader(\"UNRATE\",\"fred\",start='2000-01-01')\n",
        "oil = web.DataReader(\"DCOILBRENTEU\",\"fred\",start='2000-01-01')\n",
        "gdp = web.DataReader(\"GDP\",\"fred\",start='2000-01-01')\n",
        "t10y2y = web.DataReader(\"T10Y2Y\",\"fred\",start='2000-01-01')\n",
        "volatity = web.DataReader(\"VIXCLS\",\"fred\",start='2000-01-01')\n",
        "\n",
        "# cleaning data\n",
        "\n",
        "unrate.index = unrate.index-pd.DateOffset(days=1) # This will not cause overlap, because the data itself is one day lag\n",
        "unrate = unrate.resample('QE').last()\n",
        "gdp.index = gdp.index-pd.DateOffset(days=1) # This will not cause overlap, because the data itself is one day lag\n",
        "gdp = gdp.resample('QE').last()\n",
        "oil = oil.resample('QE').mean()\n",
        "t10y2y = t10y2y.resample('QE').mean()\n",
        "volatity = volatity.resample('QE').mean()\n",
        "\n",
        "econ = pd.concat([unrate,oil,gdp,t10y2y,volatity],axis=1)\n",
        "econ['gdp_growth'] = econ['GDP'].pct_change(fill_method=None) # econ['gdp_growth'] = econ['GDP'].diff()/econ['GDP'].shift(1), thank new features\n",
        "econ.dropna(inplace=True)\n",
        "\n",
        "for col in ['UNRATE', 'DCOILBRENTEU', 'GDP', 'gdp_growth', 'T10Y2Y', 'VIXCLS']:\n",
        "  if adfuller(econ[col])[1] > 0.05 and col != 'GDP':\n",
        "    econ[col+'_diff'] = econ[col].diff() # create first diff\n",
        "  econ[col]=econ[col].shift(-1) # avoid overlap\n",
        "\n",
        "\n",
        "def add_prefix(df, prefix): # use this function so everytime I run this part, it will not add prefix everytime\n",
        "    df.columns = [prefix + col if not col.startswith(prefix) else col for col in df.columns]\n",
        "    return df\n",
        "\n",
        "cre = add_prefix(cre, 'cre_')\n",
        "card = add_prefix(card, 'card_')\n",
        "\n",
        "df = pd.merge(cre,card,left_index=True,right_index=True)\n",
        "df = pd.merge(df,econ,left_index=True,right_index=True,how='inner') # The pct_diff starts from 2001-06-30, so use the inner to discard other rows\n",
        "\n",
        "df = df[['cre_loans', 'cre_chargeoffs', 'cre_pct', 'cre_pct_diff', \\\n",
        "         'card_loans', 'card_chargeoffs', 'card_pct', 'card_pct_diff', \\\n",
        "         'UNRATE', 'DCOILBRENTEU','DCOILBRENTEU_diff', 'GDP', 'gdp_growth', 'T10Y2Y', 'VIXCLS']]\n",
        "dfstat = df[['cre_pct_diff','card_pct_diff','UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y', 'VIXCLS']] # to conduct an AR model, we need stationary data\n"
      ],
      "metadata": {
        "id": "U9LiCFiGT5-4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "explanatory_vars = ['UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y', 'VIXCLS']\n",
        "factor_combinations = list(itertools.combinations(explanatory_vars, 3))\n",
        "\n",
        "def lag_and_regression(dependent_var, explanatory_vars, df):\n",
        "    df.loc[:, dependent_var + '_lag1'] = df[dependent_var].shift(1)\n",
        "    dfc = df.dropna()\n",
        "    X = dfc[[dependent_var + '_lag1'] + list(explanatory_vars)].dropna()\n",
        "    y = dfc[dependent_var]\n",
        "    X = sm.add_constant(X)\n",
        "    return sm.OLS(y, X).fit().rsquared\n",
        "\n",
        "\n",
        "results = [{'combo': combo,\n",
        "            'r2_cre': lag_and_regression('cre_pct_diff', combo, dfstat),\n",
        "            'r2_card': lag_and_regression('card_pct_diff', combo, dfstat)}\n",
        "           for combo in factor_combinations]\n",
        "\n",
        "top_cre_results = sorted(results, key=lambda x: x['r2_cre'], reverse=True)[:3]\n",
        "top_card_results = sorted(results, key=lambda x: x['r2_card'], reverse=True)[:3]\n",
        "print(\"Top 3 models for 'cre_pct_diff' based on R²:\")\n",
        "for result in top_cre_results:\n",
        "    print(f\"Factors: {result['combo']}, R²: {result['r2_cre']:.3f}\")\n",
        "print(\"\\nTop 3 models for 'card_pct_diff' based on R²:\")\n",
        "for result in top_card_results:\n",
        "    print(f\"Factors: {result['combo']}, R²: {result['r2_card']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-2qHQxy9PCu",
        "outputId": "cc839f01-4763-409b-bc9c-c5d80c676753"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 models for 'cre_pct_diff' based on R²:\n",
            "Factors: ('DCOILBRENTEU_diff', 'T10Y2Y', 'VIXCLS'), R²: 0.254\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'VIXCLS'), R²: 0.253\n",
            "Factors: ('DCOILBRENTEU_diff', 'gdp_growth', 'VIXCLS'), R²: 0.252\n",
            "\n",
            "Top 3 models for 'card_pct_diff' based on R²:\n",
            "Factors: ('DCOILBRENTEU_diff', 'gdp_growth', 'VIXCLS'), R²: 0.022\n",
            "Factors: ('DCOILBRENTEU_diff', 'T10Y2Y', 'VIXCLS'), R²: 0.021\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'VIXCLS'), R²: 0.020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfstat.index.freq = 'QE'\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import itertools\n",
        "\n",
        "def arima_with_factors(dependent_var, factor_combination, df):\n",
        "    X = df[list(factor_combination)]\n",
        "    model = ARIMA(df[dependent_var], exog=X, order=(1, 0, 0))\n",
        "    result = model.fit()\n",
        "    return {'combo': factor_combination,\n",
        "            'loglikelihood': result.llf,\n",
        "            'aic': result.aic,\n",
        "            'bic': result.bic}\n",
        "\n",
        "explanatory_vars = ['UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y', 'VIXCLS']\n",
        "factor_combinations = list(itertools.combinations(explanatory_vars, 3))\n",
        "results_cre = []\n",
        "results_card = []\n",
        "\n",
        "for combo in factor_combinations:\n",
        "    cre_result = arima_with_factors('cre_pct_diff', combo, dfstat)\n",
        "    card_result = arima_with_factors('card_pct_diff', combo, dfstat)\n",
        "    results_cre.append(cre_result)\n",
        "    results_card.append(card_result)\n",
        "\n",
        "best_cre_loglikelihood = sorted(results_cre, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "best_cre_aic = sorted(results_cre, key=lambda x: x['aic'])[0]\n",
        "best_cre_bic = sorted(results_cre, key=lambda x: x['bic'])[0]\n",
        "best_card_loglikelihood = sorted(results_card, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "best_card_aic = sorted(results_card, key=lambda x: x['aic'])[0]\n",
        "best_card_bic = sorted(results_card, key=lambda x: x['bic'])[0]\n",
        "\n",
        "print(\"Best model for 'cre_pct_diff' based on log-likelihood, AIC and BIC:\")\n",
        "print(f\"Factors: {best_cre_loglikelihood['combo']}, Log-likelihood: {best_cre_loglikelihood['loglikelihood']:.3f}\\\n",
        "AIC: {best_cre_aic['aic']:.3f}, BIC: {best_cre_bic['bic']:.3f}\")\n",
        "print(\"\\nBest model for 'card_pct_diff' based on log-likelihood, AIC and BIC:\")\n",
        "print(f\"Factors: {best_card_loglikelihood['combo']}, Log-likelihood: {best_card_loglikelihood['loglikelihood']:.3f}\\\n",
        "AIC: {best_card_aic['aic']:.3f}, BIC: {best_card_bic['bic']:.3f}\")\n",
        "print(\"Two methods, using OLS R-square or using loglikihood, AIC and BIC shows the same result.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kOlKvlR_cYo",
        "outputId": "bb984179-4a07-492a-ed18-86cc7a68f448"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model for 'cre_pct_diff' based on log-likelihood, AIC and BIC:\n",
            "Factors: ('DCOILBRENTEU_diff', 'T10Y2Y', 'VIXCLS'), Log-likelihood: 112.577AIC: -213.153, BIC: -199.248\n",
            "\n",
            "Best model for 'card_pct_diff' based on log-likelihood, AIC and BIC:\n",
            "Factors: ('DCOILBRENTEU_diff', 'gdp_growth', 'VIXCLS'), Log-likelihood: 21.052AIC: -30.103, BIC: -16.199\n",
            "Two methods, using OLS R-square or using loglikihood, AIC and BIC shows the same result.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pmdarima"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z28pRjiECDzw",
        "outputId": "da96c8ac-e1f6-4d7f-f70a-715e3ceee80f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.11)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.2.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (71.0.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using other models may have better result.\n",
        "from pmdarima import auto_arima\n",
        "\n",
        "def auto_arima_with_factors(dependent_var, factor_combination, df):\n",
        "    X = df[list(factor_combination)]\n",
        "    model = auto_arima(df[dependent_var], exogenous=X, seasonal=False, stepwise=True, trace=False)\n",
        "    arima_model = model.arima_res_\n",
        "    return {'combo': factor_combination,\n",
        "            'order': model.order,\n",
        "            'loglikelihood': arima_model.llf,\n",
        "            'aic': arima_model.aic,\n",
        "            'bic': arima_model.bic}\n",
        "\n",
        "results_cre, results_card = [], []\n",
        "\n",
        "for combo in factor_combinations:\n",
        "    results_cre.append(auto_arima_with_factors('cre_pct_diff', combo, dfstat))\n",
        "    results_card.append(auto_arima_with_factors('card_pct_diff', combo, dfstat))\n",
        "best_cre_ll = sorted(results_cre, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "best_card_ll = sorted(results_card, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "print(f\"Best model for 'cre_pct_diff' based on log-likelihood:\\nFactors: {best_cre_ll['combo']}, ARIMA Order: {best_cre_ll['order']}, Log-Likelihood: {best_cre_ll['loglikelihood']:.3f}, AIC: {best_cre_ll['aic']:.3f}, BIC: {best_cre_ll['bic']:.3f}\")\n",
        "print(f\"\\nBest model for 'card_pct_diff' based on log-likelihood:\\nFactors: {best_card_ll['combo']}, ARIMA Order: {best_card_ll['order']}, Log-Likelihood: {best_card_ll['loglikelihood']:.3f}, AIC: {best_card_ll['aic']:.3f}, BIC: {best_card_ll['bic']:.3f}\")\n",
        "print(\"For cre information, we have higher log-likelihood, but not for the card data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy3BYDZpKjJ3",
        "outputId": "7ba071b7-32be-4438-c7ed-81aaee6560ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model for 'cre_pct_diff' based on log-likelihood:\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth'), ARIMA Order: (1, 0, 1), Log-Likelihood: 116.355, AIC: -226.709, BIC: -219.757\n",
            "\n",
            "Best model for 'card_pct_diff' based on log-likelihood:\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth'), ARIMA Order: (0, 0, 0), Log-Likelihood: 20.410, AIC: -38.819, BIC: -36.502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3\n",
        "\n",
        "Key variables that could impact charge-off rates include interest rates, such as the Federal Funds Rate, the housing price index (HPI), consumer credit growth, the debt-to-income ratio (DTI), and initial jobless claims.\n",
        "\n",
        "-\tInterest rates or consumer price index (CPI) fluctuations can significantly affect consumers’ ability to service their credit card debts.\n",
        "-\tThe housing price index is a critical factor influencing the probability of charge-offs in commercial real estate (CRE) loans.\n",
        "-\tFor banks, when facing financial strain, they may resort to charge-offs to reduce their exposure to risky assets or to improve liquidity management.\n",
        "\n",
        "To produce accurate forecasts using these models, I would need reliable macroeconomic forecasts, insights into potential monetary and fiscal policy changes, and industry-specific trends, such as CRE vacancy rates or shifts in consumer behavior for credit cards. These inputs would significantly enhance the accuracy and reliability of future predictions."
      ],
      "metadata": {
        "id": "LHqjXrSkNO7E"
      }
    }
  ]
}