{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtVKX750mpWhj8rBBPl12I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkuSapphire/CreditRiskManagement/blob/main/Project_2_Time_Series_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2: Time Series Model\n",
        "\n",
        "By Bosen Li (bl3097), Wenyu Luo (wl2905), Edward Zhang (yz4756)"
      ],
      "metadata": {
        "id": "1QO0XIIwPhxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1\n",
        "\n",
        "In this step, we found out that both the card or the real state charge-off rate is not stationary, when we use 5% p-value as the standard. However, for the credit card loan, the p-value for ADF test is really close to 5%. Thus, we decided to keep the orignal charge-off data and conduct follow regressions for both the cre charge-off rates as well as its first difference."
      ],
      "metadata": {
        "id": "P9SFM005Qjdw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzGzIck6Pezz",
        "outputId": "4c18c350-464e-4c7b-edd0-6eeddd1bf65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is 0.489, indicating that this is not stationary.\n",
            "The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is 0.053, indicating that this is not stationary too.\n",
            "\n",
            "After taking the first difference,\n",
            "The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is 0.013, thus that this is stationary.\n",
            "The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is 0.002, thus that this is stationary too.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "cre = pd.read_excel('CRE.xlsx')\n",
        "cre.set_index('date',inplace=True)\n",
        "card = pd.read_excel('card.xlsx')\n",
        "card.set_index('date',inplace=True)\n",
        "\n",
        "# calculate pct\n",
        "cre['pct'] = 100 *  cre['chargeoffs']/cre['loans']\n",
        "card['pct'] = 100 * card['chargeoffs']/card['loans']\n",
        "\n",
        "# adf test for pct\n",
        "print(f\"The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is \\\n",
        "{adfuller(cre['pct'])[1]:.3f}, \\\n",
        "indicating that this is not stationary.\")\n",
        "print(f\"The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is \\\n",
        "{adfuller(card['pct'])[1]:.3f}, \\\n",
        "indicating that this is not stationary too.\\n\")\n",
        "\n",
        "# create first difference\n",
        "cre['pct_change'] = cre['pct'].diff()\n",
        "card['pct_change'] = card['pct'].diff()\n",
        "cre.dropna(inplace=True)\n",
        "card.dropna(inplace=True)\n",
        "\n",
        "# adf test for first difference\n",
        "print('After taking the first difference,')\n",
        "print(f\"The p-value of CRE chargeoffs percentage's Augmented Dickie Fuller test is \\\n",
        "{adfuller(cre['pct_change'])[1]:.3f}, \\\n",
        "thus that this is stationary.\")\n",
        "print(f\"The p-value of card chargeoffs percentage'sAugmented Dickie Fuller test is \\\n",
        "{adfuller(card['pct_change'])[1]:.3f}, \\\n",
        "thus that this is stationary too.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "\n",
        "In this step, we first fetch the data we need and align them up.\n",
        "- Data like unrate and gdp is reported on the first day of a month, but standing for the last month. So we move it one day frontier, which is more reasonable.\n",
        "- All data are resampled to the last day of a quarter. Accumulated data like gdp should be at then end of the quarter. Other data like oil, interest rate are highly related to price. If we are considering a portafolio, doing any kinds of forcasting, we should use the last day data because means and medium means nothing for evaluating.\n",
        "- We conduct adfuller test for each, and preserved the stationary factors, while making first difference of others, or to say, 'pct change', as suggested by professor.\n",
        "- We merge using date index, sort of like a left hand join.\n",
        "- `resample('QE')` simply means resample by quarter, using `'QE'` instead of `'Q'` to avoid annoying warnings from python.\n",
        "- Most of the factors we are using is actually stationary, so we can use them directly, yet the oil price is definetly not stationary as we guessed, so we made a first difference of it."
      ],
      "metadata": {
        "id": "nZSHv8zST6_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_datareader.data as web\n",
        "\n",
        "unrate = web.DataReader(\"UNRATE\",\"fred\",start='2000-01-01')\n",
        "oil = web.DataReader(\"DCOILBRENTEU\",\"fred\",start='2000-01-01')\n",
        "gdp = web.DataReader(\"GDP\",\"fred\",start='2000-01-01')\n",
        "t10y2y = web.DataReader(\"T10Y2Y\",\"fred\",start='2000-01-01')\n",
        "volatity = web.DataReader(\"VIXCLS\",\"fred\",start='2000-01-01')\n",
        "\n",
        "# cleaning data\n",
        "unrate.index = unrate.index-pd.DateOffset(days=1) # This will not cause overlap, because the data itself is one day lag\n",
        "unrate = unrate.resample('QE').last()\n",
        "gdp.index = gdp.index-pd.DateOffset(days=1) # This will not cause overlap, because the data itself is one day lag\n",
        "# gdp = gdp.resample('QE').last() # gdp is Q data, do not need any other change\n",
        "oil = oil.resample('QE').last()\n",
        "t10y2y = t10y2y.resample('QE').last()\n",
        "volatity = volatity.resample('QE').last()\n",
        "\n",
        "econ = pd.concat([unrate,oil,gdp,t10y2y,volatity],axis=1)\n",
        "econ['gdp_growth'] = econ['GDP'].pct_change(fill_method=None) # econ['gdp_growth'] = econ['GDP'].diff()/econ['GDP'].shift(1), thank new features\n",
        "econ.dropna(inplace=True)\n",
        "\n",
        "# print(econ.head())\n",
        "\n",
        "for col in ['UNRATE', 'DCOILBRENTEU', 'GDP', 'gdp_growth', 'T10Y2Y', 'VIXCLS']:\n",
        "  if adfuller(econ[col])[1] > 0.05 and col != 'GDP':\n",
        "    econ[col+'_diff'] = econ[col].diff() # create first diff\n",
        "  econ[col]=econ[col].shift(1) # avoid overlap\n",
        "econ.dropna(inplace=True)\n",
        "\n",
        "if (adfuller(econ['gdp_growth'])[1]) > 0.05: # not possible at all, just in case\n",
        "  econ['gdp_growth_diff'] = econ['gdp_growth'].diff()\n",
        "econ.dropna(inplace=True)\n",
        "\n",
        "# print(econ.head())\n",
        "\n",
        "def add_prefix(df, prefix): # use this function so everytime I run this part, it will not add prefix everytime\n",
        "    df.columns = [prefix + col if not col.startswith(prefix) else col for col in df.columns]\n",
        "    return df\n",
        "\n",
        "cre = add_prefix(cre, 'cre_')\n",
        "card = add_prefix(card, 'card_')\n",
        "\n",
        "df = pd.merge(cre,card,left_index=True,right_index=True)\n",
        "df = pd.merge(df,econ,left_index=True,right_index=True,how='inner') # The pct_change starts from 2001-06-30, so use the inner to discard other rows\n",
        "\n",
        "df = df[['cre_loans', 'cre_chargeoffs', 'cre_pct', 'cre_pct_change', \\\n",
        "         'card_loans', 'card_chargeoffs', 'card_pct', 'card_pct_change', \\\n",
        "         'UNRATE', 'DCOILBRENTEU','DCOILBRENTEU_diff', 'GDP', 'gdp_growth', 'T10Y2Y', 'VIXCLS']]\n",
        "dfstat = df[['card_pct','cre_pct_change','card_pct_change','UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y', 'VIXCLS']] # to conduct an AR model, we need stationary data\n"
      ],
      "metadata": {
        "id": "U9LiCFiGT5-4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AR1 using OLS\n",
        "\n",
        "We use a function `myar1` to conduct this model, and it is more readible. Because we want to use loc, so first create a copy of it, and clean them after taking the lag for ar1.\n",
        "\n",
        "The only thing we want from this model is just the r-square, so we only kept this part of the model, and then store them in a dictionary for calling.\n",
        "\n",
        "Using sort is quite easy and quick way to find the top model(s), I used to keep the top three, but to fulfill the requirements, I kept the top one. If we actually kept 3 of them, we may find out that for both cre and card charge off data, same factors can interpret the best.\n",
        "\n",
        "If the dependent variables are `pct_change`, it is clear that the r-square will be much smaller. The professor said it is some what a feature of AR1 model, which is interesting and reasonable. If we try to use the `pct` itself, the r-square will be larger because the lag as a factor will explain most of it."
      ],
      "metadata": {
        "id": "QJq1xQuo8hnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# factors are all stationary and 1Q lagged to avoid overlap\n",
        "explanatory_vars = ['UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y', 'VIXCLS']\n",
        "factor_combinations = list(itertools.combinations(explanatory_vars, 3))\n",
        "\n",
        "# ar1 is creating 1Q lag and use it as independent var\n",
        "def myar1(dependent_var, explanatory_vars, df):\n",
        "    df = df.copy()\n",
        "    df.loc[:, dependent_var + '_lag1'] = df[dependent_var].shift(1)\n",
        "    dfc = df.dropna()\n",
        "    X = dfc[[dependent_var + '_lag1'] + list(explanatory_vars)].dropna()\n",
        "    y = dfc[dependent_var]\n",
        "    X = sm.add_constant(X)\n",
        "    return sm.OLS(y, X).fit().rsquared\n",
        "\n",
        "results = [{'combo': combo,\n",
        "            'r2_cre': myar1('cre_pct_change', combo, dfstat),\n",
        "            'r2_card': myar1('card_pct_change', combo, dfstat),\n",
        "            'r2_card_ori':  myar1('card_pct', combo, dfstat)}\n",
        "           for combo in factor_combinations]\n",
        "\n",
        "top_cre_result = sorted(results, key=lambda x: x['r2_cre'], reverse=True)[:1]\n",
        "top_card_result = sorted(results, key=lambda x: x['r2_card'], reverse=True)[:1]\n",
        "top_card_ori_result = sorted(results, key=lambda x: x['r2_card_ori'], reverse=True)[:1]\n",
        "\n",
        "print(\"Top model for 'cre_pct_change' based on R²:\")\n",
        "print(f\"Factors: {top_cre_result[0]['combo']}, R²: {top_cre_result[0]['r2_cre']:.3f}\")\n",
        "print(\"\\nTop model for 'card_pct_change' based on R²:\")\n",
        "print(f\"Factors: {top_card_result[0]['combo']}, R²: {top_card_result[0]['r2_card']:.3f}\")\n",
        "print(\"\\nTop model for 'card_pct' based on R²:\")\n",
        "print(f\"Factors: {top_card_ori_result[0]['combo']}, R²: {top_card_ori_result[0]['r2_card_ori']:.3f}\")\n",
        "_=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-2qHQxy9PCu",
        "outputId": "1efd9562-7b4f-4497-8b12-208b4d9b5eec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top model for 'cre_pct_change' based on R²:\n",
            "Factors: ('DCOILBRENTEU_diff', 'T10Y2Y', 'VIXCLS'), R²: 0.410\n",
            "\n",
            "Top model for 'card_pct_change' based on R²:\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth'), R²: 0.206\n",
            "\n",
            "Top model for 'card_pct' based on R²:\n",
            "Factors: ('DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y'), R²: 0.874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some other function we tried\n",
        "\n",
        "Usually, I think we may use MLE instead of OLS to calculate ar models. So, I tried it as well. For using MLE, we won't have r-squared to explain \"how much the factors can explain the dependent var\", yet it can assess the likelihood, which is not that intuitive from my perspective."
      ],
      "metadata": {
        "id": "cxHa69-v6-BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfstat.index.freq = 'QE'\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import itertools\n",
        "\n",
        "def arima_with_factors(dependent_var, factor_combination, df):\n",
        "    X = df[list(factor_combination)]\n",
        "    model = ARIMA(df[dependent_var], exog=X, order=(1, 0, 0))\n",
        "    result = model.fit()\n",
        "    return {'combo': factor_combination,\n",
        "            'loglikelihood': result.llf,\n",
        "            'aic': result.aic,\n",
        "            'bic': result.bic}\n",
        "\n",
        "explanatory_vars = ['UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth', 'T10Y2Y', 'VIXCLS']\n",
        "factor_combinations = list(itertools.combinations(explanatory_vars, 3))\n",
        "results_cre = []\n",
        "results_card = []\n",
        "\n",
        "for combo in factor_combinations:\n",
        "    cre_result = arima_with_factors('cre_pct_change', combo, dfstat)\n",
        "    card_result = arima_with_factors('card_pct_change', combo, dfstat)\n",
        "    results_cre.append(cre_result)\n",
        "    results_card.append(card_result)\n",
        "\n",
        "best_cre_loglikelihood = sorted(results_cre, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "best_cre_aic = sorted(results_cre, key=lambda x: x['aic'])[0]\n",
        "best_cre_bic = sorted(results_cre, key=lambda x: x['bic'])[0]\n",
        "best_card_loglikelihood = sorted(results_card, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "best_card_aic = sorted(results_card, key=lambda x: x['aic'])[0]\n",
        "best_card_bic = sorted(results_card, key=lambda x: x['bic'])[0]\n",
        "\n",
        "print(\"Best model for 'cre_pct_change' based on log-likelihood, AIC and BIC:\")\n",
        "print(f\"Factors: {best_cre_loglikelihood['combo']}, Log-likelihood: {best_cre_loglikelihood['loglikelihood']:.3f}\\\n",
        "AIC: {best_cre_aic['aic']:.3f}, BIC: {best_cre_bic['bic']:.3f}\")\n",
        "print(\"\\nBest model for 'card_pct_change' based on log-likelihood, AIC and BIC:\")\n",
        "print(f\"Factors: {best_card_loglikelihood['combo']}, Log-likelihood: {best_card_loglikelihood['loglikelihood']:.3f}\\\n",
        "AIC: {best_card_aic['aic']:.3f}, BIC: {best_card_bic['bic']:.3f}\")\n",
        "print(\"Two methods, using OLS R-square or using loglikihood, AIC and BIC shows the same result.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kOlKvlR_cYo",
        "outputId": "18cc3f26-49d8-4ace-c8aa-1e2d629ad96c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model for 'cre_pct_change' based on log-likelihood, AIC and BIC:\n",
            "Factors: ('DCOILBRENTEU_diff', 'T10Y2Y', 'VIXCLS'), Log-likelihood: 118.927AIC: -225.854, BIC: -211.949\n",
            "\n",
            "Best model for 'card_pct_change' based on log-likelihood, AIC and BIC:\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth'), Log-likelihood: 28.856AIC: -45.712, BIC: -31.808\n",
            "Two methods, using OLS R-square or using loglikihood, AIC and BIC shows the same result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pmdarima # This is a package that can find the optimistic ARIMA model automatically, learnt from the apply coding for risk management seminar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z28pRjiECDzw",
        "outputId": "09210683-e5e6-4f7c-c7b2-49739a11e41b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.10/dist-packages (2.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.11)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.2.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (71.0.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using other models may have better result.\n",
        "from pmdarima import auto_arima\n",
        "\n",
        "def auto_arima_with_factors(dependent_var, factor_combination, df):\n",
        "    X = df[list(factor_combination)]\n",
        "    model = auto_arima(df[dependent_var], exogenous=X, seasonal=False, stepwise=True, trace=False)\n",
        "    arima_model = model.arima_res_\n",
        "    return {'combo': factor_combination,\n",
        "            'order': model.order,\n",
        "            'loglikelihood': arima_model.llf,\n",
        "            'aic': arima_model.aic,\n",
        "            'bic': arima_model.bic}\n",
        "\n",
        "results_cre, results_card = [], []\n",
        "\n",
        "for combo in factor_combinations:\n",
        "    results_cre.append(auto_arima_with_factors('cre_pct_change', combo, dfstat))\n",
        "    results_card.append(auto_arima_with_factors('card_pct_change', combo, dfstat))\n",
        "best_cre_ll = sorted(results_cre, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "best_card_ll = sorted(results_card, key=lambda x: x['loglikelihood'], reverse=True)[0]\n",
        "print(f\"Best model for 'cre_pct_change' based on log-likelihood:\\nFactors: {best_cre_ll['combo']}, ARIMA Order: {best_cre_ll['order']}, Log-Likelihood: {best_cre_ll['loglikelihood']:.3f}, AIC: {best_cre_ll['aic']:.3f}, BIC: {best_cre_ll['bic']:.3f}\")\n",
        "print(f\"\\nBest model for 'card_pct_change' based on log-likelihood:\\nFactors: {best_card_ll['combo']}, ARIMA Order: {best_card_ll['order']}, Log-Likelihood: {best_card_ll['loglikelihood']:.3f}, AIC: {best_card_ll['aic']:.3f}, BIC: {best_card_ll['bic']:.3f}\")\n",
        "print(\"For cre information, we have higher log-likelihood, but not for the card data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy3BYDZpKjJ3",
        "outputId": "7d5644ca-6ac2-416a-b03f-a381cf09492c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model for 'cre_pct_change' based on log-likelihood:\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth'), ARIMA Order: (1, 0, 1), Log-Likelihood: 116.355, AIC: -226.709, BIC: -219.757\n",
            "\n",
            "Best model for 'card_pct_change' based on log-likelihood:\n",
            "Factors: ('UNRATE', 'DCOILBRENTEU_diff', 'gdp_growth'), ARIMA Order: (0, 0, 0), Log-Likelihood: 20.410, AIC: -38.819, BIC: -36.502\n",
            "For cre information, we have higher log-likelihood, but not for the card data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3\n",
        "\n",
        "Key variables that I think could impact charge-off rates include interest rates, such as the Federal Funds Rate, the housing price index (HPI), consumer credit growth, the debt-to-income ratio (DTI), and initial jobless claims.\n",
        "\n",
        "-\tInterest rates or consumer price index (CPI) fluctuations can significantly affect consumers’ ability to service their credit card debts.\n",
        "-\tThe housing price index is a critical factor influencing the probability of charge-offs in commercial real estate (CRE) loans.\n",
        "-\tFor banks, when facing financial strain, they may resort to charge-offs to reduce their exposure to risky assets or to improve liquidity management.\n",
        "\n",
        "To produce accurate forecasts (for this month) using these models, firstly I need factor data of the last month, including the oil price, gdp and unemployment rate. Aside from the model, I would also need reliable macroeconomic forecasts, insights into potential monetary and fiscal policy changes, and industry-specific trends, such as CRE vacancy rates or shifts in consumer behavior for credit cards. These inputs would significantly enhance the accuracy and reliability of future predictions.\n",
        "\n",
        "Our group got 14 points last time and did not got a comment, so this time we are adding more text in our assignment. Hope this time it will help! :)"
      ],
      "metadata": {
        "id": "LHqjXrSkNO7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(0^_^0) # this can be run lol"
      ],
      "metadata": {
        "id": "tdMYGfom_JDA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}